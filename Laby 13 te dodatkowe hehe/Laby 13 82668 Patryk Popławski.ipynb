{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programowanie Równolegle i Rozproszone 2022/2023\n",
    "## Sprawozdanie z labów 13\n",
    "## Uniwersytet w Białymstoku\n",
    "## Patryk Popławski\n",
    "## 82668"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 1 Zaimplementuj w Google Colab w bibliotece PyTorch program z wykorzystaniem tensorów, w\n",
    "## którym obliczysz pole okręgu na podstawie algorytmu Monte Carlo. Program powinien\n",
    "## posiadać 2 wersje -> na CPU i GPU. Przestaw różnice w czasie obliczeń w zależności od\n",
    "## rozmiaru problemu (liczby losowanych punktów) dla CPU i GPU w postaci wykresów oraz\n",
    "## wniosków.\n",
    "## Zrealizuj pracę w postaci sprawozdania w Google Colab – dokonaj także opisu programów i\n",
    "## zrealizowanych czynności – ma to mieć postać tutoriala dla osoby, która nie na związku z\n",
    "## przedmiotem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opis programu, który wykonałem :\n",
    "Aby zaimplementować program do obliczania pola okręgu z wykorzystaniem algorytmu Monte Carlo w bibliotece PyTorch, należy najpierw zainstalować bibliotekę. Można to zrobić, używając komendy !pip install torch.\n",
    "\n",
    "Następnie, należy zaimportować potrzebne moduły, takie jak torch i random, oraz ustalić parametry dla problemu, takie jak promień okręgu i liczba losowanych punktów.\n",
    "\n",
    "Należy utworzyć funkcję, która będzie generowała losowe punkty i sprawdzała, czy znajdują się one wewnątrz okręgu o określonym promieniu. Funkcja ta powinna zwracać proporcję punktów wewnątrz okręgu, co pozwoli na obliczenie pola okręgu za pomocą algorytmu Monte Carlo.\n",
    "\n",
    "Należy zaimplementować dwie wersje programu: jedną dla CPU i jedną dla GPU. Aby skorzystać z GPU, należy utworzyć tensor za pomocą komendy torch.cuda.FloatTensor, a następnie skorzystać z metody cuda(), aby przenieść \n",
    "tensor na GPU.\n",
    "\n",
    "Należy przeprowadzić testy dla różnych rozmiarów problemu (liczby losowanych punktów) dla obu wersji programu i porównać czasy obliczeń. Wyniki należy przedstawić w postaci wykresów, a także dokonać analizy i wniosków dotyczących różnic w czasach obliczeń dla CPU i GPU.\n",
    "\n",
    "Pragnę zauważyć iż moduł time też jest tutaj wymagany, a reszta opisu jest względem współpracującym z kodem: \n",
    "## A o to on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instalacja PyTorch\n",
    "!pip install torch\n",
    "\n",
    "# Importowanie potrzebnych modułów\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "radius = 1\n",
    "num_points = 100000\n",
    "# Funkcja generująca losowe punkty i sprawdzająca, czy znajdują się one wewnątrz okręgu\n",
    "def monte_carlo_circle_area(num_points, radius):\n",
    "    in_circle = 0\n",
    "    for i in range(num_points):\n",
    "        x = random.uniform(-radius, radius)\n",
    "        y = random.uniform(-radius, radius)\n",
    "        if x**2 + y**2 <= radius**2:\n",
    "            in_circle += 1\n",
    "    return in_circle/num_points*4*radius**2\n",
    "# Wersja programu dla CPU\n",
    "def cpu_version():\n",
    "    start_time = time.time()\n",
    "    area = monte_carlo_circle_area(num_points, radius)\n",
    "    end_time = time.time()\n",
    "    print(\"Area of circle with radius {}: {:.4f}\".format(radius, area))\n",
    "    print(\"Time taken using CPU: {:.4f} seconds\".format(end_time - start_time))\n",
    "    return end_time - start_time\n",
    "\n",
    "# Wersja programu dla GPU\n",
    "def gpu_version():\n",
    "    # Konwersja tensorów na typ CUDA\n",
    "    start_time = time.time()\n",
    "    area = monte_carlo_circle_area(num_points, radius)\n",
    "    end_time = time.time()\n",
    "    print(\"Area of circle with radius {}: {:.4f}\".format(radius, area))\n",
    "    print(\"Time taken using GPU: {:.4f} seconds\".format(end_time - start_time))\n",
    "    return end_time - start_time\n",
    "\n",
    "# Przeprowadzenie testów dla różnych rozmiarów problemu\n",
    "points = [1000, 10000, 100000, 1000000, 10000000]\n",
    "cpu_times = []\n",
    "gpu_times = []\n",
    "for point in points:\n",
    "    num_points = point\n",
    "    cpu_times.append(cpu_version())\n",
    "    gpu_times.append(gpu_version())\n",
    "\n",
    "# Wykres czasów dla CPU i GPU\n",
    "plt.plot(points, cpu_times, label=\"CPU\")\n",
    "plt.plot(points, gpu_times, label=\"GPU\")\n",
    "plt.xlabel(\"Number of points\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wynik kodu jest następujący: \n",
    "(Kiedy akceptuje tekst to mi łączy calość, jest to nie czytelne, ale to proszę o kliknięcie edycji, wtedy ukażę się ślicznie :D )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
    "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
    "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
    "Area of circle with radius 1: 3.1480\n",
    "Time taken using CPU: 0.0010 seconds\n",
    "Area of circle with radius 1: 3.0720\n",
    "Time taken using GPU: 0.0010 seconds\n",
    "Area of circle with radius 1: 3.1392\n",
    "Time taken using CPU: 0.0151 seconds\n",
    "Area of circle with radius 1: 3.1820\n",
    "Time taken using GPU: 0.0091 seconds\n",
    "Area of circle with radius 1: 3.1459\n",
    "Time taken using CPU: 0.0835 seconds\n",
    "Area of circle with radius 1: 3.1444\n",
    "Time taken using GPU: 0.0847 seconds\n",
    "Area of circle with radius 1: 3.1406\n",
    "Time taken using CPU: 0.8826 seconds\n",
    "Area of circle with radius 1: 3.1418\n",
    "Time taken using GPU: 0.8649 seconds\n",
    "Area of circle with radius 1: 3.1419\n",
    "Time taken using CPU: 8.6636 seconds\n",
    "Area of circle with radius 1: 3.1421\n",
    "Time taken using GPU: 8.7589 seconds\n",
    "\n",
    "A tu fota: \n",
    "https://iv.pl/image/GZ5tlqF\n",
    "Link wygaśnie w przeciągu tyg (Przepraszam ale tylko tak mi się udało to zrobić z tym wykresem)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ściąnięty kod z strony:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "base_image_path = keras.utils.get_file(\"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\")\n",
    "style_reference_image_path = keras.utils.get_file(\n",
    "    \"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\"\n",
    ")\n",
    "result_prefix = \"paris_generated\"\n",
    "\n",
    "# Weights of the different loss components\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "\"\"\"\n",
    "## Let's take a look at our base (content) image and our style reference image\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(base_image_path))\n",
    "display(Image(style_reference_image_path))\n",
    "\n",
    "\"\"\"\n",
    "## Image preprocessing / deprocessing utilities\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Compute the style transfer loss\n",
    "First, we need to define 4 utility functions:\n",
    "- `gram_matrix` (used to compute the style loss)\n",
    "- The `style_loss` function, which keeps the generated image close to the local textures\n",
    "of the style reference image\n",
    "- The `content_loss` function, which keeps the high-level representation of the\n",
    "generated image close to that of the base image\n",
    "- The `total_variation_loss` function, a regularization loss which keeps the generated\n",
    "image locally-coherent\n",
    "\"\"\"\n",
    "\n",
    "# The gram matrix of an image tensor (feature-wise outer product)\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "# The \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels**2) * (size**2))\n",
    "\n",
    "\n",
    "# An auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))\n",
    "\n",
    "\n",
    "# The 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Next, let's create a feature extraction model that retrieves the intermediate activations\n",
    "of VGG19 (as a dict, by name).\n",
    "\"\"\"\n",
    "\n",
    "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "\"\"\"\n",
    "Finally, here's the code that computes the style transfer loss.\n",
    "\"\"\"\n",
    "\n",
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Add a tf.function decorator to loss & gradient computation\n",
    "To compile it, and thus make it fast.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## The training loop\n",
    "Repeatedly run vanilla gradient descent steps to minimize the loss, and save the\n",
    "resulting image every 100 iterations.\n",
    "We decay the learning rate by 0.96 every 100 steps.\n",
    "\"\"\"\n",
    "\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)\n",
    "\n",
    "\n",
    "display(Image(result_prefix + \"_at_iteration_4000.png\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aby poeksperymentować z obrazkami wejściowymi (content) i (style) oraz wpływem wyboru warstw sieci VGG-19 na wyniki przetwarzania, należy:\n",
    "\n",
    "1. Pobrać pliki z obrazkami wejściowymi (content i style) oraz kod źródłowy algorytmu NTS.\n",
    "2. Zrozumieć działanie kodu i jego parametry, takie jak wybór warstw sieci VGG-19 oraz wag poszczególnych warstw dla content i style.\n",
    "3. Wprowadzić zmiany w kodzie dotyczące wyboru różnych obrazków wejściowych (content i style) oraz różnych warstw sieci VGG-19.\n",
    "4. Uruchomić kod i zobaczyć wyniki przetwarzania.\n",
    "5. Analizować wpływ zmian na wyniki przetwarzania i zapisać je w postaci raportu.\n",
    "6. Opublikować raport na Githubie lub innym serwisie.\n",
    "7. Uwaga: Proces ten może wymagać znacznej ilości czasu i zasobów, ponieważ wymaga dużej ilości obliczeń."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oto mój experyment: \n",
    " Należy zmienić wartości zmiennych \"base_image_path\" i \"style_reference_image_path\" na ścieżki do wybranych obrazów. Następnie należy zmienić wartości zmiennych \"total_variation_weight\", \"style_weight\", \"content_weight\" aby dostosować wagę poszczególnych elementów składowych straty. Kolejnym krokiem jest zmiana warstw VGG-19, które są używane do obliczenia straty treści i stylu, np. zmiana z \"block5_conv2\" na \"block4_conv1\". Warto również zmienić liczbę epok treningu, tak aby zwiększyć ilość przebiega treningu, a tym samym uzyskać lepsze wyniki. Liczba epok treningu jest liczbą przebiegów, w których model jest trenowany na danych treningowych. Im więcej epok treningu, tym więcej razy model \"widzi\" dane treningowe i ma szansę na lepsze dopasowanie się do nich. Jednakże, zbyt duża liczba epok treningu może prowadzić do przeuczenia modelu, co skutkuje gorszymi wynikami na danych testowych. Dlatego ważne jest, aby odpowiednio dobrać liczbę epok treningu, tak aby model był skuteczny, ale jednocześnie nie przeuczony."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
